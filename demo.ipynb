{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MIT License (MIT)\n",
    "\n",
    "Copyright (c) 2018 Lisong Guo <lisong.guo@me.com>\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "this software and associated documentation files (the \"Software\"), to deal in\n",
    "the Software without restriction, including without limitation the rights to\n",
    "use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract\n",
    "\n",
    "This notebook is intended to showcase how to use the MNL (Multinomial Logistic Regression) model to predict the booking probability for each option within a session.\n",
    "\n",
    "One can find the sample training and testing data under the `data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pprint \n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15,6\n",
    "rcParams['figure.dpi'] = 100\n",
    "rcParams['savefig.dpi'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# import the model and all the auxiliary functions\n",
    "from MNL import *\n",
    "from MNL_aux import *\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num features: 17\n",
      "========================\n",
      "{'loss': 'MaxLogLikelihood', 'optimizer': 'Adam', 'learning_rate': 0.001, 'momentum': 0, 'weight_decay': 0, 'epochs': 10, 'early_stop_min_delta': 0.0001, 'patience': 5, 'gpu': True, 'verbose': 1, 'l1_loss_weight': 0, 'l2_loss_weight': 0, 'save_gradients': False, 'MNL_features': ['deptime_inbound_cos2p', 'deptime_inbound_cos4p', 'deptime_inbound_sin2p', 'deptime_inbound_sin4p', 'deptime_outbound_cos2p', 'deptime_outbound_cos4p', 'deptime_outbound_sin2p', 'deptime_outbound_sin4p', 'price_elasticity', 'reco_contains_CX', 'reco_contains_MH', 'reco_contains_OD', 'reco_contains_PG', 'reco_contains_SQ', 'reco_contains_TG', 'reco_contains_VN', 'rescaled_reco_eft']}\n",
      "========================\n",
      "epoch: 0  loss: 205.86882631565956 best_loss: 1000000000000000.0\n",
      "epoch: 1  loss: 132.92307192913773 best_loss: 205.86882631565956\n",
      "epoch: 2  loss: 131.047915600501 best_loss: 132.92307192913773\n",
      "epoch: 3  loss: 130.27623517127373 best_loss: 131.047915600501\n",
      "epoch: 4  loss: 129.84892685085276 best_loss: 130.27623517127373\n",
      "epoch: 5  loss: 129.5809260059052 best_loss: 129.84892685085276\n"
     ]
    }
   ],
   "source": [
    "\n",
    "TRAIN_CONFIG = {\n",
    "    #'MNL_features': MNL_features,\n",
    "    \n",
    "    # when absent, by default, use all the features within the training data\n",
    "    #'MNL_features': MNL_features,\n",
    "    \n",
    "    # options: BinaryCrossEntropy, MaxLogLikelihood\n",
    "    'loss':  'MaxLogLikelihood',\n",
    "    #'loss':  'BinaryCrossEntropy',\n",
    "    \n",
    "    'optimizer': 'Adam',  # options:  Adam, RMSprop, SGD, LBFGS.\n",
    "    # Adam would converge much faster\n",
    "    # LBFGS is a very memory intensive optimizer (it requires additional param_bytes * (history_size + 1) bytes).\n",
    "    # If it doesnâ€™t fit in memory try reducing the history size, or use a different algorithm.\n",
    "    # By default, history_size == 100\n",
    "    'learning_rate': 1e-3, # Applicable to Adam, SGD, and LBFGS\n",
    "    # The learning_rate parameter seems essential to LBFGS, which converges in two epochs.\n",
    "    #  So far, learning_rate == 0.1 seems to be ok for LBFGS\n",
    "    \n",
    "    #'momentum': 0.9,  # applicable to SGD, RMSprop\n",
    "    'momentum': 0,  # applicable to SGD, RMSprop\n",
    "    \n",
    "    # The resulting model seems to be more balanced, i.e. no extreme large/small weights,\n",
    "    #  although one might not have the most ideal performance, i.e. high top_5_rank etc.\n",
    "    'weight_decay': 0, # Applicable to Adam, RMSprop and SGD\n",
    "    \n",
    "    'epochs': 10,\n",
    "    'early_stop_min_delta': 1e-4,\n",
    "    'patience': 5,\n",
    "    \n",
    "    'gpu': True,  # luckily, running on GPU is faster than CPU in this case.\n",
    "    \n",
    "    # level of logging, 0: no log,  1: print epoch related logs;  2: print session related logs\n",
    "    'verbose': 1,\n",
    "    \n",
    "    # Adding the regularization degredates the performance of model\n",
    "    #   which might suggests that the model is still underfitting, not overfitting.\n",
    "    'l1_loss_weight': 0,  # e.g. 0.001 the regularization that would marginalize the weights\n",
    "    'l2_loss_weight': 0,\n",
    "    \n",
    "    # flag indicates whether to save gradients during the training\n",
    "    'save_gradients': False\n",
    "}\n",
    "\n",
    "\n",
    "# set random seed for reproduceability\n",
    "np.random.seed(17)\n",
    "torch.manual_seed(17)\n",
    "\n",
    "df_train = pd.read_csv('data/train_SINBKK_RT_B.csv')\n",
    "\n",
    "# Create a brand-new model\n",
    "model_tuple, loss_list = run_training(df_train, TRAIN_CONFIG)\n",
    "\n",
    "# Continue training on the existing model\n",
    "#model_tuple, loss_list = run_training(df_mlogit, TRAIN_CONFIG, model_tuple)\n",
    "\n",
    "\n",
    "# unzip the tuple\n",
    "(model, loss, optimizer) = model_tuple\n",
    "\n",
    "\n",
    "# plot the evolution of loss\n",
    "plot_loss(loss_list)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
